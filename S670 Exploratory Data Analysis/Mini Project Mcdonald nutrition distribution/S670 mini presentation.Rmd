---
output:
  pdf_document: default
  html_document: default
---


# 22 Fall S670 Mini presentation

## PCA basic concepts and steps

Step 1 - Normalizing Data
Due to the varied scales of this information, conducting PCA with such data could result in biased outcomes. This is where data normalization becomes crucial. It ensures that each attribute contributes equally, preventing the dominance of one variable over others. For each variable, normalization involves subtracting its mean and dividing by its standard deviation. Also, PCA only operates for numerical values.

Step 2 - Computing Covariance Matrix
As the name implies, this step involves calculating the covariance matrix from the normalized data. This symmetric matrix assigns each element (i, j) to the covariance between variables i and j.

Step 3 - Eigenvectors and Eigenvalues
In geometric terms, an eigenvector denotes a direction, such as "vertical" or "90 degrees." Conversely, an eigenvalue is a numerical representation of the variance present in the data for a specific direction. Each eigenvector corresponds to a unique eigenvalue.

Step 4 - Selecting Principal Components
The number of pairs of eigenvectors and eigenvalues matches the number of variables in the data. In the case of data comprising only monthly expenses, age, and rating, there will be three pairs. However, not all pairs are relevant. The eigenvector with the highest eigenvalue corresponds to the first principal component, the second principal component aligns with the eigenvector possessing the second highest eigenvalue, and so forth.

Step 5 - Transforming Data in a New Dimensional Space
This step involves repositioning the original data into a new subspace defined by the principal components. This reorientation is achieved by multiplying the original data by the previously computed eigenvectors.

It's crucial to note that this transformation doesn't alter the original data but instead provides a new perspective for better data representation.

```{r}
mc <- read.csv("menu.csv")
# https://www.kaggle.com/datasets/mcdonalds/nutrition-facts/ <- data reference site
head(mc)
```
##EDA work


```{r}
library(ggplot2)

ggplot(mc, aes(x = Sugars, y = Calories, group=Category))+ 
         geom_point(aes( color=Category), size=2)+
  theme(legend.position="top")
```
###Calculate the Correlation Matrix
While the covariance matrix is discussed in the preceding five steps, utilizing the correlation matrix is also a viable option. The correlation matrix can be computed using the cor() function from the corrr package. For improved visualization, the ggcorrplot() function can then be applied.

```{r}

library(ggcorrplot)
library("FactoMineR")
mc2 <- mc[ c(4,6,10,11,13, 15,19,20)]

cormatrix <- cor(mc2)
ggcorrplot(cormatrix)
```

```{r}
library(psych)
#correlation ellipses
pairs.panels(mc2, 
             method = "pearson", 
             hist.col = "blue",
             density = TRUE,  
             ellipses = TRUE 
             )
```







```{r}
library(ggplot2)
library(dplyr)
findout_outlier <- function(x) {
  return(x < quantile(x, .25) - 1.5*IQR(x) | x > quantile(x, .75) + 1.5*IQR(x))
}


mc <- mc |>
        group_by(Category) |>
        mutate(outlier = ifelse(findout_outlier(Calories), Item, NA))

ggplot(mc, aes(x=Category, y=Calories,fill=Category)) +
  geom_boxplot() +
  geom_text(aes(label=outlier), na.rm=TRUE, hjust=-.5)+ theme(axis.text.x = element_text(angle = 30, vjust = 0.5, hjust=1))
```
##Application of PCA


```{r}
pca2 <- princomp(cormatrix)
summary(pca2)
pca2$loadings[, 1:2]


```
### explanation for summary of the 'pca'
Each principal component accounts for a percentage of the total variance within the dataset. In the Cumulative Proportion section, the first principal component elucidates almost 80% of the total variance. This signifies that nearly three-quarters of the data across the set of 9 variables can be encapsulated by the first principal component alone. The second principal component contributes to explaining 10.41% of the total variance.

The cumulative proportion represented by Comp.1 and Comp.2 encompasses almost 91% of the total variance. This indicates that utilizing the first two principal components is sufficient for an accurate representation of the data.

While having the first two components is advantageous, understanding their significance is essential. This can be addressed by examining how they correlate with each column through the loadings of each principal component.


### explanation for the pca2$loadings
The loading matrix reveals that the first principal component exhibits significantly positive values for Total.Fat, Cholesterol, Sodium and Protein. Conversely, Trans.Fat, Carbohydrates and sugars demonstrate relatively negative values. This indicates that items with elevated consumption of sodium, while those with lower intake Sugars.

Regarding the second principal component, it manifests high positive values for Calories and Sugars. This suggests that the patterns of items are notably influenced by their Calories amount, with emphasizing sugar consumption and inland regions favoring a total fat in items.



```{r}
#visualization package
library(factoextra)
#Apply PCA
pca <- prcomp(mc2, scale = TRUE)
#Visualize eigenvalues from scree plot. Each principal component shows the percentage of variances.
fviz_eig(pca)
```

### Visualization of Principal Components

While the previous analysis of the loading matrix provided a comprehensive understanding of the relationship between the first two principal components and the attributes in the data, it may lack visual appeal. To address this, several standard visualization strategies are available to help users gain insights into the data, starting with the scree plot.

####Scree Plot
The scree plot is employed to visualize the significance of each principal component and aids in determining the number of principal components to retain. To generate the scree plot, the fviz_eig() function can be utilized.

```{r}
fviz_cos2(pca2, choice = "var", axes = 1:2)
```
#### Contribution of Each Variable(Above):

The objective of the third visualization is to assess the representation of each variable in a given principal component. This measure of representation is known as Cos2, representing the square cosine. It is computed using the `fviz_cos2` function.

- A low Cos2 value indicates that the variable is not well-represented by that component.
- A high Cos2 value implies a strong representation of the variable on that component.

The code above calculates the square cosine value for each variable concerning the first two principal components. The highest Cos2 makes the most significant contributions to PC1 and PC2.

```{r}
# Graph of the variables
fviz_pca_var(pca2, col.var = "blue")
fviz_eig(pca2, addlabels = TRUE)

```

#### Interpretation of Scree Plot
The scree plot displays the eigenvalues in a descending curve, arranged from the highest to the lowest. The first two components emerge as the most substantial, encapsulating nearly 91% of the total information within the data.

#### Biplot of Attributes(Above)
The biplot offers a visual representation that facilitates the observation of similarities and dissimilarities between samples. Furthermore, it illustrates the influence of each attribute on each of the principal components. This comprehensive view aids in understanding the overall structure of the data and the role played by individual attributes in shaping it.

The biplot provides three key:

1.Positive Correlation Grouping
   - Variables that cluster together on the biplot are positively correlated. This observation aligns with their high values in the loading matrix concerning the first principal component.

2. Distance from Origin
   - The distance from the origin signifies the quality of representation for a variable. Items positioned farther from the origin, are better represented. The magnitude of the distance indicates the strength of representation.

3. Negative Correlation Placement
   - Variables with a negative correlation are positioned on opposite sides of the biplot's origin. This spatial arrangement visually emphasizes the negative correlation between these variables.
```{r}
fviz_pca_var(pca2, col.var = "cos2",
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
            repel = TRUE)


```



#### Biplot Combined with Cos2(Above)

The biplot and attribute importance, can be integrated into a single biplot. In this visualization, attributes with similar Cos2 scores are assigned similar colors. The adjustment is made using the `fviz_pca_var` function.

- Red Attributes (High Cos2): Sugar and Sodium are highlighted in red, signifying high Cos2 scores. These attributes contribute significantly to both PC1 and PC2.

- Yellow Attributes (Mid Cos2): Total Fat and Protein appear in yellow. Their intermediate Cos2 scores indicate a moderate contribution to PC1 and PC2.

- Blue Attributes (Low Cos2): Tran.Fat and Cholesterol are represented in black, suggesting lower Cos2 scores and a relatively weaker contribution to PC1 and PC2.









```{r}


#Visualization of Variable Relationships: In a graphical representation of variables, those with positive correlations tend to align on the same side of the plot, while variables with negative correlations point towards opposite sides of the graph. This visual depiction allows for a quick assessment of the relationships between variables and their orientations in the multidimensional space.
fviz_pca_var(pca, col.var = "contrib", 
             gradient.cols = c("lightblue3", "yellow2", "pink2"), repel = TRUE)
```


```{r}
library(factoextra)
# Computing Eigenvalues 
eig.val <- get_eigenvalue(pca)
eig.val
  
# Computing Variables
variance <- get_pca_var(pca)
variance$coord          
variance$contrib        
variance$cos2           
index_pca <- get_pca_ind(pca)
index_pca$coord          
index_pca$contrib        # Contributions of the PCs
index_pca$cos2           # degree of representation 

```



