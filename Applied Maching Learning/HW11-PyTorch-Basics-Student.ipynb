{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02bea0b-d3e2-47b1-bd07-8ec6f40c7c27",
   "metadata": {},
   "source": [
    "**NOTE:** <br>\n",
    "For best results, please use the provided Docker container and run this notebook inside that.\n",
    "You can also run the notebook in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b188e-a62a-4b80-8c61-41e87f8f72bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homegrown Linear Regression (LR) via numpy\n",
    "\n",
    "Here, we will implement linear regression (a single layer network, with an architecture \"2-1\", corresponding to a model with 2 inputs, and a single output node) using array programming, and also in network fashion using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array class, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; but it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a single-layer network (linear regression) to random data by manually implementing the forward and backward (gradient descent) passes through the network using numpy operations. The following code shows how to accomplish this:\n",
    "\n",
    "\n",
    "The below code trains a linear regression model from random data (so the model learns nothing really; it is just an exercise).\n",
    "\n",
    "\n",
    "**QUESTION** :Run the below code and modify as needed and answer the following question. What is the value of the gradient for the first iteration of gradient descent after running the above code?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ec8ced-f056-4c66-82a7-a117fc48b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, MSE: 2.389670625903901\n",
      "Epoch:0, Gradient: [[ 0.587]\n",
      " [-2.399]]\n",
      "--------------------\n",
      "Epoch:1, MSE: 2.383574809721735\n",
      "Epoch:1, Gradient: [[ 0.587]\n",
      " [-2.395]]\n",
      "--------------------\n",
      "Epoch:2, MSE: 2.377502693303068\n",
      "Epoch:2, Gradient: [[ 0.586]\n",
      " [-2.39 ]]\n",
      "--------------------\n",
      "Epoch:3, MSE: 2.3714541834963083\n",
      "Epoch:3, Gradient: [[ 0.585]\n",
      " [-2.385]]\n",
      "--------------------\n",
      "Epoch:4, MSE: 2.3654291875192457\n",
      "Epoch:4, Gradient: [[ 0.585]\n",
      " [-2.38 ]]\n",
      "--------------------\n",
      "Epoch:5, MSE: 2.359427612957574\n",
      "Epoch:5, Gradient: [[ 0.584]\n",
      " [-2.375]]\n",
      "--------------------\n",
      "Epoch:6, MSE: 2.3534493677634245\n",
      "Epoch:6, Gradient: [[ 0.583]\n",
      " [-2.371]]\n",
      "--------------------\n",
      "Epoch:7, MSE: 2.3474943602538985\n",
      "Epoch:7, Gradient: [[ 0.583]\n",
      " [-2.366]]\n",
      "--------------------\n",
      "Epoch:8, MSE: 2.3415624991096156\n",
      "Epoch:8, Gradient: [[ 0.582]\n",
      " [-2.361]]\n",
      "--------------------\n",
      "Epoch:9, MSE: 2.3356536933732595\n",
      "Epoch:9, Gradient: [[ 0.581]\n",
      " [-2.357]]\n",
      "--------------------\n",
      "Weights after training: ,[[ 0.208]\n",
      " [-1.222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# m_rows is batch size; \n",
    "# D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 2, 1\n",
    "#m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "np.random.seed(seed=42) #fix the seed\n",
    "# Create random input and output data\n",
    "X_train = np.random.randn(m_rows, D_in)\n",
    "y_train = np.random.randn(m_rows, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "W1 = np.random.randn(D_in, D_out) #[w0, w1, w2, ...w999]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for epoch in range(10):  #Gradient descent\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = X_train.dot(W1)\n",
    "    #y_pred = X_train @ W1 \n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y_train).mean()\n",
    "    print(f\"Epoch:{epoch}, MSE: {loss}\")\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    err = 2.0 * (y_pred - y_train)\n",
    "    grad_W1 = X_train.T.dot(err)/m_rows  #weighted sum of the train data\n",
    "    print(f\"Epoch:{epoch}, Gradient: {np.round(grad_W1, 3)}\")\n",
    "    \n",
    "    # Update weights via gradient descent\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    print('--------------------')\n",
    "    \n",
    "print(f\"Weights after training: ,{np.round(W1, 3)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773e621-a5b0-42ec-b7c5-0525c0056196",
   "metadata": {},
   "source": [
    "# Linear regression using tensors and autograd\n",
    "\n",
    "\n",
    "Traditionally, say using Numpy, we had to manually implement both the forward and backward passes to train a  linear regression model (aka a single-layered neural network). Manually implementing the backward pass is not a big deal for a linear regression model or for a small/shallow network, but this can quickly get very tricky for larger multilayers networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality.\n",
    "\n",
    "When using autograd,\n",
    "* **the forward pass of your network (code for making a prediction) will define a computational graph;**\n",
    "* nodes in the graph will be Tensors,\n",
    "* and edges will be functions that produce output Tensors from input Tensors.\n",
    "\n",
    "PyTorch builds up a graph as you compute the forward pass through the network, and one call to **backward()** on some result node (loss function) then augments each intermediate node in the graph with the gradient of the result node with respect to that intermediate node.\n",
    "\n",
    "This sounds complicated, but it’s pretty simple to use in practice. We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then x.data is a Tensor, and x.grad is another Variable holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "PyTorch Variables are PyTorch Tensors variables.\n",
    "\n",
    "Here we use PyTorch tensor variables and autograd to implement our single-layer network (linear regression model); now we no longer need to manually implement the backward pass through the network:\n",
    "\n",
    "**Question** Run the below code and modify as needed to get the value of the first element of the learnt linear regression model. What is the value of the first element of the learnt linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3960f26-95a9-4650-894c-0379f1c59a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10, MSE: 829.467\n",
      "Epoch:20, MSE: 828.909\n",
      "Epoch:30, MSE: 828.352\n",
      "Epoch:40, MSE: 827.794\n",
      "Epoch:50, MSE: 827.237\n",
      "------------------------\n",
      "tensor([1.8839], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    Y_pred = X_train.matmul(W1)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (Y_pred - Y_train).pow(2).mean()\n",
    "    if (epoch +1 )%10 ==0:  #print every 10 epochs\n",
    "        print(f\"Epoch:{epoch+1}, MSE: {loss.data.numpy():.6}\")\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call W1.grad  will be Variables holding the gradient\n",
    "    # of the loss with respect to W1.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; W1.data and W2.data are Tensors,\n",
    "    # W1.grad are Variables and W1.grad.data aare\n",
    "    # Tensors.\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    # Manually zero the gradients after updating weights\n",
    "    W1.grad.data.zero_()\n",
    "\n",
    "print('------------------------')    \n",
    "# print the first value of the learnt linear regression model weight\n",
    "\n",
    "# TODO: Add code below inside the print statement\n",
    "print(W1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a0079-a4e7-4e12-a870-814e10ca9442",
   "metadata": {},
   "source": [
    "# PyTorch: optim\n",
    "\n",
    "\n",
    "Up to this point we have updated the weights of our models by manually mutating the .data member for Variables holding learnable parameters.\n",
    "\n",
    "`W1.data -= learning_rate * W1.grad.data`\n",
    "\n",
    "This is not a huge burden for simple optimization algorithms like stochastic gradient descent on a single layer network (such as a multiple linear regession (MLR) model), but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc. More on these optimizers later.\n",
    "\n",
    "The **optim** package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.\n",
    "\n",
    "The following example brings these two highly scaleable concepts of computational graphs and optimization to life:\n",
    "\n",
    "* we will use the nn package to define our our linear regression module,\n",
    "* and we will use optimize the model using the **Adam** algorithm, a variant of stochastic gradient descent. The **Adam** algorithm comes as part of the the **optim** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be74e30-04bd-4526-98ba-9504a8406d70",
   "metadata": {},
   "source": [
    "# Building PyTorch Networks using the Sequential API\n",
    "\n",
    "The **Sequential** class allows us to build a neural network in PyTorch in a high-level quick and modular manner.\n",
    "\n",
    "* The Sequential class allows us to build PyTorch neural networks on-the-fly without having to build an explicit class.\n",
    "* This make it much easier to rapidly build networks and allows us to skip over the step where we implement the forward() method. When we use the sequential way of building a PyTorch network, we construct the forward() method implicitly by defining our network's architecture sequentially.\n",
    " \n",
    "\n",
    "Here we use the Sequential() API to build a single-layered neural network (upon closer inspection you see it is a linear regression model) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82d3513-7873-4d25-bd5f-83b30b175e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, MSE: 1.48583221\n",
      "Epoch:10, MSE: 1.24699187\n",
      "Epoch:20, MSE: 1.03472447\n",
      "Epoch:30, MSE: 0.849931717\n",
      "Epoch:40, MSE: 0.691670775\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# D_out is output dimension.\n",
    "m_rows, D_in, D_out = 64, 1000, 1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "X_train = torch.randn((m_rows, D_in), requires_grad=False) #use (m_rows, D_in) or m_rows, D_in\n",
    "Y_train = torch.randn(m_rows, D_out, requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "# NOT Need as we are using the nn package\n",
    "#  W1 = torch.randn(D_in, D_out,  requires_grad=True)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(  #  X_train @ W1\n",
    "    torch.nn.Linear(D_in, D_out),   # X.matmul(W1)\n",
    ")\n",
    "# loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(50):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "    if epoch % 10 == 0:\n",
    "         print(f\"Epoch:{epoch}, MSE: {loss.item():.9}\")\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4281d-4b27-430b-88ae-fdf0a530e14f",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "A single layered  neural network consisting of an input layer (not counted as a layer) and an output layer  is essentially just a linear regression model. A two-layered network consisting for an input layer (not counted as a layer), a hidden layer, and an output layer is still essentially just a linear regression model. Both these networks are just linear transformations of the inputs. To make these networks nonlinear (and enhance their predictive power), we introduce  a non-linear activation function (that acts element-wise on the linear transformed inputs).\n",
    "\n",
    " \n",
    "\n",
    "A popular activation function is the Sigmoid function. It is one of the most widely used non-linear activation function. Sigmoid transforms the values between the range 0 and 1. Here is the mathematical expression for sigmoid:\n",
    "\n",
    "`f(x) = 1/(1+e^-x)`\n",
    "\n",
    "``` import numpy as np\n",
    "def sigmoid_function(x):\n",
    "    z = (1/(1 + np.exp(-x)))\n",
    "    return z\n",
    "```  \n",
    "    \n",
    "**ReLU**\n",
    "\n",
    "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. This means that the neurons will only be deactivated if the output of the linear transformation is less than 0. \n",
    "\n",
    "```\n",
    "def relu_function(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "```\n",
    "\n",
    "`relu_function(7), relu_function(-7). # returns (7, 0) as a result`\n",
    "\n",
    "For the negative part of the input domain, you will notice that the value is zero.\n",
    "\n",
    "In PyTorch ReLU is available as a built in layer via  torch.nn.ReLU().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10745a70-4023-48de-a52b-e67f4ef50e5b",
   "metadata": {},
   "source": [
    "**The following are examples of neural networks consisting of input, hidden, and output layers:**\n",
    "\n",
    "```    \n",
    "network1 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network2 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.Linear(out_features, 1) # X.matmul(W2) + b2 )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "network3 = nn.Sequential(\n",
    "    nn.Linear(in_features, out_features), # X.matmul(W1) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(in_features, out_features), # X.matmul(W2) + b1\n",
    "    nn.ReLU(), #Nonlinear activation function.  nn.ReLU(X.matmul(W1) + b1) \n",
    "    nn.Linear(out_features, 1) # X.matmul(W3) + b2 )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da53625-3312-45fe-ad11-2f75a14c340d",
   "metadata": {},
   "source": [
    "# Boston house price regression via Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19c8e00-fbb0-49f2-baa7-36cdee912f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63bb7ff-4e9e-4360-9fbe-ef5571d9f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for better reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# if GPU, use cuda-enabled GPU device else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "# X.shape\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform validation set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "boston_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "\n",
    "\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_boston = torch.utils.data.DataLoader(boston_validation, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_boston_seq_model( \n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 1  # Output layer neurons - depend on what you're trying to predict, here, just a single value\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "    \n",
    "    # print(arch_string)\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_layer_neurons)):\n",
    "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
    "        layers.append(torch.nn.Linear(prev, curr))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    # MSE loss scaffolding layer\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print('-'*50)\n",
    "    print('Model summary:')\n",
    "    print(model)\n",
    "    summary(model, (1, 13))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Print device used - CPU or GPU\n",
    "    print(f\"Using {device}...\")\n",
    "    \n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, torch.unsqueeze(target.float(), dim=1))    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        train_mse = np.round(running_loss/count, 3)\n",
    "        return train_mse\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        for i,data in enumerate(data_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, torch.unsqueeze(target.float(), dim=1))           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        valid_mse = np.round(overall_loss/count, 3)\n",
    "        # print(f\"{tag} MSE loss: {valid_mse:.3f}\")\n",
    "        return valid_mse\n",
    "        \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_mse = train_epoch(epoch, model, loss_fn, optimizer, trainloader_boston)\n",
    "        valid_mse = evaluate_model(epoch, model, loss_fn, optimizer, validloader_boston, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}: Train MSE: {train_mse}\\t Validation MSE: {valid_mse}\")\n",
    "    print(\"-\"*50)\n",
    "    test_mse = evaluate_model(epoch, model, loss_fn, opt, testloader_boston, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, train_mse, valid_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb2c668-1e34-46ee-bbdf-44f9d4b09519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model summary:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 32]             448\n",
      "              ReLU-2                [-1, 1, 32]               0\n",
      "            Linear-3                [-1, 1, 16]             528\n",
      "              ReLU-4                [-1, 1, 16]               0\n",
      "            Linear-5                 [-1, 1, 8]             136\n",
      "              ReLU-6                 [-1, 1, 8]               0\n",
      "            Linear-7                 [-1, 1, 1]               9\n",
      "================================================================\n",
      "Total params: 1,121\n",
      "Trainable params: 1,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Using cpu...\n",
      "Epoch 1: Train MSE: 592.192\t Validation MSE: 569.957\n",
      "Epoch 2: Train MSE: 589.849\t Validation MSE: 650.528\n",
      "Epoch 3: Train MSE: 574.795\t Validation MSE: 512.235\n",
      "Epoch 4: Train MSE: 568.819\t Validation MSE: 547.085\n",
      "Epoch 5: Train MSE: 567.431\t Validation MSE: 523.641\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Valid MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-32-16-8-1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>567.431</td>\n",
       "      <td>523.641</td>\n",
       "      <td>489.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                      Optimizer Epochs Train MSE  \\\n",
       "0        13-32-16-8-1  <class 'torch.optim.sgd.SGD'>      5   567.431   \n",
       "\n",
       "  Valid MSE Test MSE  \n",
       "0   523.641  489.015  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve a low MSE value\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "learning_rate = 1e-3\n",
    "hidden_layer_neurons = [32, 16, 8]\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 5\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, train_mse, valid_mse, test_mse = run_boston_seq_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")\n",
    "    \n",
    "try: bostonLog \n",
    "except : bostonLog  = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Train MSE\",\n",
    "        \"Valid MSE\",\n",
    "        \"Test MSE\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "bostonLog.loc[len(bostonLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{train_mse}\",\n",
    "    f\"{valid_mse}\", \n",
    "    f\"{test_mse}\",\n",
    "]\n",
    "\n",
    "bostonLog "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b1a6c-d008-4dab-ae54-f0896b360a73",
   "metadata": {},
   "source": [
    "# Perform Classification on Iris dataset via Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f40c1042-89f0-4f43-810e-6714220a031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn import datasets\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle = True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform validation set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "iris_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "iris_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "iris_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# print(X_test.shape)\n",
    "\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "trainloader_iris = torch.utils.data.DataLoader(iris_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_iris = torch.utils.data.DataLoader(iris_validation, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_iris = torch.utils.data.DataLoader(iris_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_iris_model(\n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 3  # Output layer neurons - depend on what you're trying to predict, here, 3 classes\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_layer_neurons)):\n",
    "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
    "        layers.append(torch.nn.Linear(prev, curr))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "    \n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        loss = np.round(running_loss/count, 3)\n",
    "        \n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for i,data in enumerate(data_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, target)           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        loss = np.round(overall_loss/count, 3)\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        return loss, accuracy\n",
    "        \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_loss, train_accuracy = train_epoch(epoch, model, loss_fn, optimizer, trainloader_iris)\n",
    "        valid_loss, valid_accuracy = evaluate_model(epoch, model, loss_fn, optimizer, validloader_iris, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}: Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
    "    print(\"-\"*50)\n",
    "    test_loss, test_accuracy = evaluate_model(epoch, model, loss_fn, opt, testloader_iris, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, train_accuracy, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "143f5f73-58c4-45c5-9d5a-0eeb9e22c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Epoch 1: Train Accuracy: 0.346\t Validation Accuracy: 0.35\n",
      "Epoch 2: Train Accuracy: 0.346\t Validation Accuracy: 0.35\n",
      "Epoch 3: Train Accuracy: 0.346\t Validation Accuracy: 0.35\n",
      "Epoch 4: Train Accuracy: 0.346\t Validation Accuracy: 0.35\n",
      "Epoch 5: Train Accuracy: 0.346\t Validation Accuracy: 0.35\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4-32-16-8-3</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>34.599999999999994%</td>\n",
       "      <td>35.0%</td>\n",
       "      <td>26.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                      Optimizer Epochs  \\\n",
       "0         4-32-16-8-3  <class 'torch.optim.sgd.SGD'>      5   \n",
       "\n",
       "        Train accuracy Validation accuracy Test accuracy  \n",
       "0  34.599999999999994%               35.0%         26.1%  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve larger train/test accuracy\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "\n",
    "learning_rate = 1e-3\n",
    "hidden_layer_neurons = [32, 16, 8]\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 5\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, train_accuracy, valid_accuracy, test_accuracy = run_iris_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")\n",
    "    \n",
    "\n",
    "try: irisLog \n",
    "except : irisLog = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Train accuracy\",\n",
    "        \"Validation accuracy\",\n",
    "        \"Test accuracy\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "irisLog.loc[len(irisLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{train_accuracy * 100}%\",\n",
    "    f\"{valid_accuracy * 100}%\",\n",
    "    f\"{test_accuracy * 100}%\",\n",
    "]\n",
    "\n",
    "irisLog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9f920-8f12-4b1b-b701-2f4ce24740a2",
   "metadata": {},
   "source": [
    "# Boston house price regression via OOP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0d2e890-f784-405f-a897-4e661c7d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "import torch\n",
    "#import torchvision\n",
    "import torch.utils.data\n",
    "#import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set random seed for better reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# if GPU, use cuda-enabled GPU device else CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "# X.shape\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform validation set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "boston_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "\n",
    "\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 16\n",
    "\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_boston = torch.utils.data.DataLoader(boston_validation, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_boston_oop_model(\n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 1  # Output layer neurons - depend on what you're trying to predict, here, just a single value\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "\n",
    "    # Use the OOP API to define a deep neural network model\n",
    "    #\n",
    "    class BaseModel(nn.Module):\n",
    "        \"\"\"Custom module for a simple  regressor\"\"\"\n",
    "        def __init__(self, in_features, hidden_neurons=[16, 8, 4], n_output=1):\n",
    "            super(BaseModel, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(in_features, hidden_neurons[0])   # 1st hidden layer\n",
    "            \n",
    "            # All other intermediate hidden layers\n",
    "            self.intermediate_layers = torch.nn.ModuleList()\n",
    "            for i in range(1, len(hidden_neurons)):\n",
    "                prev, curr = hidden_neurons[i - 1], hidden_neurons[i]\n",
    "                self.intermediate_layers.append(torch.nn.Linear(prev, curr))\n",
    "            # print(self.intermediate_layers)\n",
    "             \n",
    "            self.fc_output = torch.nn.Linear(hidden_neurons[-1], n_output) # output layer\n",
    "\n",
    "        def forward(self, x):\n",
    "            # print(self.intermediate_layers)\n",
    "            x = F.relu(self.fc1(x))   # activation function for 1st hidden layer\n",
    "            \n",
    "            # The intermediate layers\n",
    "            for i in range(len(self.intermediate_layers)):\n",
    "                x = F.relu(self.intermediate_layers[i](x))\n",
    "                \n",
    "            x = self.fc_output(x)  # Output layer without activation\n",
    "            return x\n",
    "\n",
    "    # Print device used - CPU or GPU\n",
    "    print(f\"Using {device}...\")\n",
    "\n",
    "    # create classifier and optimizer objects\n",
    "    model = BaseModel(in_features=D_in, hidden_neurons=hidden_layer_neurons, n_output=D_out)\n",
    "    model.to(device) # put on GPU before setting up the optimizer\n",
    "\n",
    "    # Here, summary will not reflect the actual number of layers as we have a list of intermediate_layers as apposed to a specific layer like self.fc1\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    summary(model, (1, 13))\n",
    "    print('-'*50)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, torch.unsqueeze(target.float(), dim=1))    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        train_mse = np.round(running_loss/count, 3)\n",
    "        return train_mse\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        for i,data in enumerate(data_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, torch.unsqueeze(target.float(), dim=1))           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        valid_mse = np.round(overall_loss/count, 3)\n",
    "        # print(f\"{tag} MSE loss: {valid_mse:.3f}\")\n",
    "        return valid_mse\n",
    "        \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_mse = train_epoch(epoch, model, loss_fn, optimizer, trainloader_boston)\n",
    "        valid_mse = evaluate_model(epoch, model, loss_fn, optimizer, validloader_boston, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}: Train MSE: {train_mse}\\t Validation MSE: {valid_mse}\")\n",
    "    print(\"-\"*50)\n",
    "    test_mse = evaluate_model(epoch, model, loss_fn, opt, testloader_boston, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, train_mse, valid_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "880d6090-48dd-4647-b232-0e2c10d473d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu...\n",
      "--------------------------------------------------\n",
      "Model:\n",
      "BaseModel(\n",
      "  (fc1): Linear(in_features=13, out_features=32, bias=True)\n",
      "  (intermediate_layers): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): Linear(in_features=16, out_features=8, bias=True)\n",
      "  )\n",
      "  (fc_output): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 32]             448\n",
      "            Linear-2                [-1, 1, 16]             528\n",
      "            Linear-3                 [-1, 1, 8]             136\n",
      "            Linear-4                 [-1, 1, 1]               9\n",
      "================================================================\n",
      "Total params: 1,121\n",
      "Trainable params: 1,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Epoch 1: Train MSE: 592.192\t Validation MSE: 569.957\n",
      "Epoch 2: Train MSE: 589.849\t Validation MSE: 650.528\n",
      "Epoch 3: Train MSE: 574.795\t Validation MSE: 512.235\n",
      "Epoch 4: Train MSE: 568.819\t Validation MSE: 547.085\n",
      "Epoch 5: Train MSE: 567.431\t Validation MSE: 523.641\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-32-16-8-1</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>567.431</td>\n",
       "      <td>523.641</td>\n",
       "      <td>489.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                      Optimizer Epochs Train MSE  \\\n",
       "0        13-32-16-8-1  <class 'torch.optim.sgd.SGD'>      5   567.431   \n",
       "\n",
       "  Validation MSE Test MSE  \n",
       "0        523.641  489.015  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve a low MSE value\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "\n",
    "learning_rate = 1e-3\n",
    "hidden_layer_neurons = [32, 16, 8]\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 5\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, train_mse, valid_mse, test_mse = run_boston_oop_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")\n",
    "    \n",
    "try: bostonOopLog \n",
    "except : bostonOopLog  = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Train MSE\",\n",
    "        \"Validation MSE\",\n",
    "        \"Test MSE\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "bostonOopLog.loc[len(bostonOopLog )] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{train_mse}\",\n",
    "    f\"{valid_mse}\",\n",
    "    f\"{test_mse}\",\n",
    "]\n",
    "\n",
    "bostonOopLog "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50556ec6-d85c-43cb-81e9-da82fba243f6",
   "metadata": {},
   "source": [
    "# Perform Classification on HCDR dataset via Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85026e-5625-47a8-b74f-6523bd195527",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Note:\n",
    "Have a look at the Home Credit Default Risk data on Kaggle: https://www.kaggle.com/competitions/home-credit-default-risk/overview <br>\n",
    "\n",
    "There are multiple .csv files provided for this challenge on Kaggle. For this question, we'll be only using the **application_train.csv** to predict TARGET : 1 or 0. <br>\n",
    "\n",
    "**DATASET LINK:** Download the .zip of the .csv file from here: https://www.dropbox.com/s/rmqsrhhc8qhyq50/application_train.csv.zip?dl=0\n",
    "\n",
    "Save the **application_train.csv.zip** in the same directory you're running this notebook in. Please ensure that the .zip is downloaded and then run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac06dc7b-a73b-4660-9237-d731d1bae92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 172 kB of archives.\n",
      "After this operation, 559 kB of additional disk space will be used.\n",
      "Err:1 http://deb.debian.org/debian stretch/main amd64 unzip amd64 6.0-21+deb9u2\n",
      "  404  Not Found\n",
      "E: Failed to fetch http://deb.debian.org/debian/pool/main/u/unzip/unzip_6.0-21+deb9u2_amd64.deb  404  Not Found\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
     ]
    }
   ],
   "source": [
    "!apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b78c6265-bb89-4472-8db4-b1d08e18d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: unzip: not found\n"
     ]
    }
   ],
   "source": [
    "!unzip -o application_train.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80d60d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (307511, 121) (307511,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "hcdr_application = pd.read_csv(\"application_train.csv\")\n",
    "X = hcdr_application.drop('TARGET', axis = 1)\n",
    "y = hcdr_application.TARGET\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle = True)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "## Scaling\n",
    "numerical_features = X.select_dtypes(include = ['int64','float64']).columns\n",
    "numerical_features = numerical_features.tolist()\n",
    "\n",
    "num_pipeline =Pipeline([('std',StandardScaler()),\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_features = X.select_dtypes(include = ['object']).columns\n",
    "categorical_features = categorical_features.tolist()\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "features = numerical_features + categorical_features\n",
    "\n",
    "data_pipeline = ColumnTransformer([\n",
    "       (\"num_pipeline\", num_pipeline, numerical_features),\n",
    "       (\"cat_pipeline\", cat_pipeline, categorical_features)],\n",
    "        remainder='drop',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "X_train = data_pipeline.fit_transform(X_train)\n",
    "X_validation = data_pipeline.transform(X_validation) #Transform validation set with the same constants\n",
    "X_test = data_pipeline.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "\n",
    "y_train = y_train.to_numpy()\n",
    "y_validation = y_validation.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_valid_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_valid_tensor = torch.from_numpy(y_validation)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "hcdr_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "hcdr_valid = torch.utils.data.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "hcdr_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "# create dataloader\n",
    "# DataLoader is implemented in PyTorch, which will return an iterator to iterate training data by batch.\n",
    "train_batch_size = 96\n",
    "valid_test_batch_size = 64\n",
    "trainloader_hcdr = torch.utils.data.DataLoader(hcdr_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "validloader_hcdr = torch.utils.data.DataLoader(hcdr_valid, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "testloader_hcdr = torch.utils.data.DataLoader(hcdr_test, batch_size=valid_test_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#\n",
    "# Method to create, define and run a deep neural network model\n",
    "#\n",
    "def run_hcdr_model(\n",
    "    hidden_layer_neurons=[32, 16, 8],\n",
    "    opt=optim.SGD,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    \n",
    "    D_in = X_test.shape[1]  # Input layer neurons depend on the input dataset shape\n",
    "    D_out = 2  # Output layer neurons - depend on what you're trying to predict, here, 2 classes: 0 and 1\n",
    "    \n",
    "    str_neurons = [str(h) for h in hidden_layer_neurons]\n",
    "    arch_string = f\"{D_in}-{'-'.join(str_neurons)}-{D_out}\"\n",
    "    \n",
    "    layers = [\n",
    "        torch.nn.Linear(D_in, hidden_layer_neurons[0]),  # X.matmul(W1)\n",
    "        nn.ReLU(),  # ReLU( X.matmul(W1))\n",
    "    ]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_layer_neurons)):\n",
    "        prev, curr = hidden_layer_neurons[i - 1], hidden_layer_neurons[i]\n",
    "        layers.append(torch.nn.Linear(prev, curr))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "    \n",
    "    # Add final layer\n",
    "    layers.append(nn.Linear(hidden_layer_neurons[-1], D_out)) # Relu( X.matmul(W1)).matmul(W2))\n",
    "    \n",
    "    # Use the nn package to define our model and loss function.\n",
    "    # use the sequential API makes things simple\n",
    "    model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # use Cross Entropy and SGD optimizer.\n",
    "    loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "    optimizer = opt(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #summary(model, (4, 20))\n",
    "    print('-'*50)\n",
    "    print('Model:')\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "    \n",
    "    '''\n",
    "    Training Process:\n",
    "        Load a batch of data.\n",
    "        Zero the grad.\n",
    "        Predict the batch of the data through net i.e forward pass.\n",
    "        Calculate the loss value by predict value and true value.\n",
    "        Backprop i.e get the gradient with respect to parameters\n",
    "        Update optimizer i.e gradient update\n",
    "    '''\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        # dataset API gives us pythonic batching \n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)        \n",
    "            # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = model(inputs.float()) #prediction over the input data\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(preds, target)    #mean loss for this batch\n",
    "\n",
    "            loss.backward() #calculate nabla_w\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()  #update W\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        loss = np.round(running_loss/count, 3)\n",
    "        \n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "    #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "    def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        y_pred = []\n",
    "        epoch_target = []\n",
    "        for i,data in enumerate(data_loader):\n",
    "            inputs, target = data[0].to(device), data[1].to(device)                \n",
    "            preds = model(inputs.float())      \n",
    "\n",
    "            loss = loss_fn(preds, target)           # compute loss value\n",
    "\n",
    "            overall_loss += (loss.item())  # compute total loss to save to logs\n",
    "            y_pred.extend(torch.argmax(preds, dim=1).tolist())\n",
    "            epoch_target.extend(target.tolist())\n",
    "            count += 1\n",
    "\n",
    "        # compute mean loss\n",
    "        loss = np.round(overall_loss/count, 3)\n",
    "        #accuracy\n",
    "        correct = (np.array(y_pred) == np.array(epoch_target))\n",
    "        accuracy = correct.sum() / correct.size\n",
    "        accuracy = np.round(accuracy, 3)\n",
    "        return loss, accuracy\n",
    "        \n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(f\"Epoch {epoch+1}\")\n",
    "        train_loss, train_accuracy = train_epoch(epoch, model, loss_fn, optimizer, trainloader_hcdr)\n",
    "        valid_loss, valid_accuracy = evaluate_model(epoch, model, loss_fn, optimizer, validloader_hcdr, tag = \"Validation\")\n",
    "        print(f\"Epoch {epoch+1}: Train Accuracy: {train_accuracy}\\t Validation Accuracy: {valid_accuracy}\")\n",
    "    print(\"-\"*50)\n",
    "    test_loss, test_accuracy = evaluate_model(epoch, model, loss_fn, opt, testloader_hcdr, tag=\"Test\")\n",
    "    \n",
    "    return arch_string, train_accuracy, valid_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325a162-0eeb-4b07-b016-98d1b152c042",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "**The following cell might take relatively longer to run owing to the larger size of the dataset and the parameters you choose for the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfa10c27-8efd-4356-8357-cfcf2334bb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=245, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Epoch 1: Train Accuracy: 0.822\t Validation Accuracy: 0.916\n",
      "Epoch 2: Train Accuracy: 0.92\t Validation Accuracy: 0.916\n",
      "Epoch 3: Train Accuracy: 0.92\t Validation Accuracy: 0.916\n",
      "Epoch 4: Train Accuracy: 0.92\t Validation Accuracy: 0.916\n",
      "Epoch 5: Train Accuracy: 0.92\t Validation Accuracy: 0.916\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture string</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Valid accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245-32-16-8-2</td>\n",
       "      <td>&lt;class 'torch.optim.sgd.SGD'&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>92.0%</td>\n",
       "      <td>91.60000000000001%</td>\n",
       "      <td>91.9%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Architecture string                      Optimizer Epochs Train accuracy  \\\n",
       "0       245-32-16-8-2  <class 'torch.optim.sgd.SGD'>      5          92.0%   \n",
       "\n",
       "       Valid accuracy Test accuracy  \n",
       "0  91.60000000000001%         91.9%  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# NOTE: Run this cell however number of times you want to achieve larger train/test accuracy\n",
    "# Experiment with different arguments to the function\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "#==================================================#\n",
    "#    Modify START   #\n",
    "#==================================================#\n",
    "'''\n",
    "(hidden_layers_neurons) - A list of the number of neurons in the hidden layers in order. DEFAULT: [32, 16, 8] => 1st hidden layer: 32 neurons, 2nd: 16, 3rd: 8\n",
    "(opt) - The optimizer function to use: SGD, Adam, etc.,  DEFAULT: optim.SGD\n",
    "(epochs) - The total number of epochs to train your model for,  DEFAULT: 5\n",
    "(learning_rate) - The learning rate to take the gradient descent step with\n",
    "'''\n",
    "\n",
    "learning_rate = 1e-3\n",
    "hidden_layer_neurons = [32, 16, 8]\n",
    "opt = optim.SGD  # optim.SGD, Optim.Adam, etc.\n",
    "epochs = 5\n",
    "\n",
    "#==================================================#\n",
    "#    Modify END #\n",
    "#==================================================#\n",
    "\n",
    "arch_string, train_accuracy, valid_accuracy, test_accuracy = run_hcdr_model(\n",
    "    hidden_layer_neurons,\n",
    "    opt,\n",
    "    epochs,\n",
    "    learning_rate\n",
    ")\n",
    "    \n",
    "\n",
    "try: hcdrLog \n",
    "except : hcdrLog = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"Architecture string\", \n",
    "        \"Optimizer\", \n",
    "        \"Epochs\", \n",
    "        \"Train accuracy\",\n",
    "        \"Valid accuracy\",\n",
    "        \"Test accuracy\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "hcdrLog.loc[len(hcdrLog)] = [\n",
    "    arch_string, \n",
    "    f\"{opt}\", \n",
    "    f\"{epochs}\", \n",
    "    f\"{train_accuracy * 100}%\",\n",
    "    f\"{valid_accuracy * 100}%\",\n",
    "    f\"{test_accuracy * 100}%\",\n",
    "]\n",
    "\n",
    "hcdrLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5bf93-a1ed-4722-a5e0-78b4d99e47b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
